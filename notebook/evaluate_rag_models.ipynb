{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Evaluation (Notebook) — Taiwan Industry VAlue Chain QA\n",
        "\n",
        "This notebook mirrors the execution order / flow of `evaluate_rag_models.py`:\n",
        "\n",
        "1. Environment + imports\n",
        "2. Load YAML config\n",
        "3. Build Knowledge Base from individual chain JSON files\n",
        "4. Initialize embeddings + FAISS vector store\n",
        "5. Initialize LLM provider\n",
        "6. Retrieve + generate per sample\n",
        "7. Parse response → compute metrics\n",
        "8. Save results + print summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Environment + Imports (mirrors script top section)\n",
        "import os, re, json, time, asyncio, random, warnings\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import yaml\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "load_dotenv()\n",
        "\n",
        "# Make the uploaded libs importable\n",
        "import sys\n",
        "LIB_DIR = \"../src/utils\"\n",
        "if LIB_DIR not in sys.path:\n",
        "    sys.path.insert(0, LIB_DIR)\n",
        "\n",
        "# Uploaded evaluation libs\n",
        "from config import ModelConfig, RAGConfig as LibRAGConfig\n",
        "from metrics import calculate_metrics, calculate_average_precision\n",
        "from providers import get_provider, list_available_providers\n",
        "\n",
        "print(\"✓ Imports OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Load evaluation_config.yaml (optional defaults / overrides)\n",
        "\n",
        "YAML_PATH = Path(\"../config/evaluation_config.yaml\")\n",
        "yaml_cfg = {}\n",
        "if YAML_PATH.exists():\n",
        "    with open(YAML_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        yaml_cfg = yaml.safe_load(f) or {}\n",
        "    print(f\"✓ Loaded YAML: {YAML_PATH}\")\n",
        "else:\n",
        "    print(f\"(i) YAML not found at {YAML_PATH}, using defaults in notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Notebook parameters (replace argparse)\n",
        "# You can edit these directly.\n",
        "\n",
        "# ---- Dataset ----\n",
        "DATASET_PATH = yaml_cfg.get(\"dataset\", \"../datasets/demo/qa/firm_chains_qa_local.jsonl\")\n",
        "\n",
        "# ---- LLM provider/model ----\n",
        "PROVIDER = yaml_cfg.get(\"provider\", \"ollama\")  # openai / anthropic / google / ollama\n",
        "MODEL_NAME = yaml_cfg.get(\"model\", \"gemma3:27b-it-q8_0\")\n",
        "\n",
        "# ---- Generation params ----\n",
        "TEMPERATURE = float(yaml_cfg.get(\"temperature\", 0.0))\n",
        "MAX_TOKENS = int(yaml_cfg.get(\"max_tokens\", 500))\n",
        "TIMEOUT = int(yaml_cfg.get(\"timeout\", 30))\n",
        "\n",
        "# ---- RAG params ----\n",
        "EMBEDDING_PROVIDER = yaml_cfg.get(\"embedding_provider\", \"ollama\")  # openai / huggingface / ollama / google\n",
        "EMBEDDING_MODEL = yaml_cfg.get(\"embedding_model\", None)  # if None, use provider default\n",
        "TOP_K = int(yaml_cfg.get(\"top_k\", 5))\n",
        "SCORE_THRESHOLD = float(yaml_cfg.get(\"score_threshold\", 0.0))\n",
        "DATA_DIR = yaml_cfg.get(\"data_dir\", \"../datasets/demo/individual_chains\")\n",
        "\n",
        "# ---- Sampling ----\n",
        "MAX_SAMPLES = yaml_cfg.get(\"max_samples\", None)  # e.g. 100\n",
        "SAMPLE_RATE = float(yaml_cfg.get(\"sample_rate\", 1.0))\n",
        "\n",
        "# ---- Save results ----\n",
        "SAVE_RESULTS = bool(yaml_cfg.get(\"save_results\", True))\n",
        "RESULTS_DIR = Path(yaml_cfg.get(\"results_dir\", \"../results\"))\n",
        "\n",
        "# ---- Ollama reasoning switch (not in uploaded LibRAGConfig, but kept to mirror your script) ----\n",
        "# For Ollama only: if False, we will try to reduce/disable thinking via provider extra_params.\n",
        "ENABLE_REASONING = bool(yaml_cfg.get(\"enable_reasoning\", False))\n",
        "\n",
        "print(\"✓ Parameters set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) LangChain imports for RAG components (FAISS + embeddings)\n",
        "try:\n",
        "    from langchain_core.documents import Document\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    from langchain_ollama import OllamaEmbeddings\n",
        "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        \"Missing packages for RAG. Install: \"\n",
        "        \"pip install langchain langchain-community faiss-cpu langchain-openai \"\n",
        "        \"langchain-huggingface langchain-ollama langchain-google-genai\"\n",
        "    ) from e\n",
        "\n",
        "print(\"✓ LangChain RAG imports OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Knowledge Base (mirrors TaiwanValueChainKnowledgeBase in your script)\n",
        "\n",
        "class TaiwanValueChainKnowledgeBase:\n",
        "    \"\"\"Knowledge base for Taiwan value chain data.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str = \"../datasets/demo/individual_chains\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.documents: List[Document] = []\n",
        "        self.company_to_chains: Dict[str, set] = {}\n",
        "        self.chain_to_companies: Dict[str, set] = {}\n",
        "        self._load_data()\n",
        "    \n",
        "    def _load_data(self):\n",
        "        print(f\"Loading Taiwan value chain data from {self.data_dir}...\")\n",
        "        if not self.data_dir.exists():\n",
        "            raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
        "        \n",
        "        json_files = list(self.data_dir.glob(\"*.json\"))\n",
        "        if not json_files:\n",
        "            raise FileNotFoundError(f\"No JSON files found in {self.data_dir}\")\n",
        "        \n",
        "        print(f\"Found {len(json_files)} value chain files\")\n",
        "        for json_file in json_files:\n",
        "            try:\n",
        "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                    chain_data = json.load(f)\n",
        "                self._process_value_chain(chain_data)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading {json_file}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"Loaded {len(self.documents)} documents\")\n",
        "        print(f\"Company-Chain mappings: {len(self.company_to_chains)}\")\n",
        "        print(f\"Chain-Company mappings: {len(self.chain_to_companies)}\")\n",
        "        \n",
        "        sample_companies = list(self.company_to_chains.keys())[:10]\n",
        "        print(f\"Sample companies in KB: {sample_companies}\")\n",
        "        test_companies = [\"91APP*-KY\", \"91APP\", \"ACpay\", \"IET-KY\", \"台積電\"]\n",
        "        found_companies = [c for c in test_companies if c in self.company_to_chains]\n",
        "        print(f\"Test companies found: {found_companies}\")\n",
        "        if found_companies:\n",
        "            first_company = found_companies[0]\n",
        "            chains_for_company = self.company_to_chains[first_company]\n",
        "            print(f\"Example: {first_company} belongs to {len(chains_for_company)} chains\")\n",
        "    \n",
        "    def _process_value_chain(self, chain_data: Dict[str, Any]):\n",
        "        try:\n",
        "            chain_name = chain_data.get(\"title\", \"\")\n",
        "            introduction = chain_data.get(\"introduction\", \"\")\n",
        "            if not chain_name:\n",
        "                return\n",
        "            \n",
        "            # main chain document\n",
        "            main_doc_content = f\"產業鏈名稱: {chain_name}\\n\"\n",
        "            if introduction:\n",
        "                intro_text = introduction[:1000] + \"...\" if len(introduction) > 1000 else introduction\n",
        "                main_doc_content += f\"介紹: {intro_text}\\n\"\n",
        "            self.documents.append(\n",
        "                Document(\n",
        "                    page_content=main_doc_content,\n",
        "                    metadata={\"type\": \"value_chain\", \"chain_name\": chain_name},\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            if chain_name not in self.chain_to_companies:\n",
        "                self.chain_to_companies[chain_name] = set()\n",
        "            \n",
        "            chains = chain_data.get(\"chains\", [])\n",
        "            for chain_section in chains:\n",
        "                self._process_chain_section(chain_section, chain_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error processing chain data: {e}\")\n",
        "    \n",
        "    def _process_chain_section(self, chain_section: Dict[str, Any], chain_name: str):\n",
        "        try:\n",
        "            section_title = chain_section.get(\"title\", \"\")\n",
        "            companies = chain_section.get(\"companies\", [])\n",
        "            section_content = f\"產業鏈: {chain_name}\\n類別: {section_title}\\n\"\n",
        "            companies_in_section = set()\n",
        "            \n",
        "            for company_item in companies:\n",
        "                if isinstance(company_item, dict):\n",
        "                    if \"detailed_data\" in company_item:\n",
        "                        detailed_data = company_item[\"detailed_data\"]\n",
        "                        sub_companies = detailed_data.get(\"companies\", [])\n",
        "                        for company_info in sub_companies:\n",
        "                            if isinstance(company_info, dict):\n",
        "                                company_name = company_info.get(\"name\", \"\")\n",
        "                                is_foreign = company_info.get(\"is_foreign\", False)\n",
        "                                if company_name:\n",
        "                                    companies_in_section.add(company_name)\n",
        "                                    self._add_company_mapping(company_name, chain_name, section_title, is_foreign)\n",
        "                    elif \"name\" in company_item:\n",
        "                        company_name = company_item.get(\"name\", \"\")\n",
        "                        is_foreign = company_item.get(\"is_foreign\", False)\n",
        "                        if company_name:\n",
        "                            companies_in_section.add(company_name)\n",
        "                            self._add_company_mapping(company_name, chain_name, section_title, is_foreign)\n",
        "            \n",
        "            if companies_in_section:\n",
        "                section_content += f\"包含公司: {', '.join(sorted(companies_in_section))}\\n\"\n",
        "            \n",
        "            self.documents.append(\n",
        "                Document(\n",
        "                    page_content=section_content,\n",
        "                    metadata={\n",
        "                        \"type\": \"category\",\n",
        "                        \"chain_name\": chain_name,\n",
        "                        \"category\": section_title,\n",
        "                        \"company_count\": len(companies_in_section),\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error processing chain section: {e}\")\n",
        "    \n",
        "    def _add_company_mapping(self, company_name: str, chain_name: str, category: str, is_foreign: bool):\n",
        "        try:\n",
        "            if company_name not in self.company_to_chains:\n",
        "                self.company_to_chains[company_name] = set()\n",
        "            self.company_to_chains[company_name].add(chain_name)\n",
        "            self.chain_to_companies.setdefault(chain_name, set()).add(company_name)\n",
        "            \n",
        "            comp_content = (\n",
        "                f\"公司名稱: {company_name}\\n\"\n",
        "                f\"產業鏈: {chain_name}\\n\"\n",
        "                f\"類別: {category}\\n\"\n",
        "                f\"外國公司: {'是' if is_foreign else '否'}\\n\"\n",
        "            )\n",
        "            self.documents.append(\n",
        "                Document(\n",
        "                    page_content=comp_content,\n",
        "                    metadata={\n",
        "                        \"type\": \"company\",\n",
        "                        \"company_name\": company_name,\n",
        "                        \"chain_name\": chain_name,\n",
        "                        \"category\": category,\n",
        "                        \"is_foreign\": is_foreign,\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error adding company mapping: {e}\")\n",
        "\n",
        "print(\"✓ KnowledgeBase class ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Embeddings initializer (mirrors your script)\n",
        "\n",
        "def initialize_embeddings(embedding_provider: str, embedding_model: Optional[str] = None, base_url: Optional[str] = None):\n",
        "    default_embedding_models = {\n",
        "        \"openai\": \"text-embedding-3-small\",\n",
        "        \"huggingface\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "        \"ollama\": \"qwen3-embedding:4b\",\n",
        "        \"google\": \"models/gemini-embedding-001\",\n",
        "    }\n",
        "    model = embedding_model or default_embedding_models.get(embedding_provider, default_embedding_models[\"huggingface\"])\n",
        "\n",
        "    if embedding_provider == \"openai\":\n",
        "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"OpenAI API key required for embeddings (OPENAI_API_KEY)\")\n",
        "        return OpenAIEmbeddings(model=model, openai_api_key=api_key), model\n",
        "\n",
        "    if embedding_provider == \"huggingface\":\n",
        "        return HuggingFaceEmbeddings(model_name=model), model\n",
        "\n",
        "    if embedding_provider == \"ollama\":\n",
        "        # OllamaEmbeddings uses local Ollama server\n",
        "        base = \"http://localhost:11434\"\n",
        "        return OllamaEmbeddings(model=model, base_url=base), model\n",
        "\n",
        "    if embedding_provider == \"google\":\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Google API key required for embeddings (GOOGLE_API_KEY)\")\n",
        "        return GoogleGenerativeAIEmbeddings(model=model, google_api_key=api_key), model\n",
        "\n",
        "    raise ValueError(f\"Unsupported embedding provider: {embedding_provider}\")\n",
        "\n",
        "print(\"✓ Embeddings initializer ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) RAG Provider wrapper: uses uploaded providers.py for LLM generation\n",
        "#    but keeps the same retrieve-and-generate logic as your script.\n",
        "\n",
        "class RAGModelProvider:\n",
        "    def __init__(\n",
        "        self,\n",
        "        provider: str,\n",
        "        model_name: str,\n",
        "        temperature: float,\n",
        "        max_tokens: int,\n",
        "        timeout: int,\n",
        "        base_url: Optional[str],\n",
        "        enable_reasoning: bool,\n",
        "        knowledge_base: TaiwanValueChainKnowledgeBase,\n",
        "        embedding_provider: str,\n",
        "        embedding_model: Optional[str],\n",
        "        top_k: int,\n",
        "        score_threshold: float,\n",
        "        extra_params: Optional[Dict[str, Any]] = None,\n",
        "    ):\n",
        "        self.provider_name = provider\n",
        "        self.model_name = model_name\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.top_k = top_k\n",
        "        self.score_threshold = score_threshold\n",
        "        self.base_url = base_url\n",
        "\n",
        "        # ---- Initialize LLM using uploaded providers.py ----\n",
        "        llm_extra = dict(extra_params or {})\n",
        "        if provider == \"ollama\":\n",
        "            # Mirror your script's reasoning switch behavior as best-effort.\n",
        "            # providers.py currently sets think automatically; we override if requested.\n",
        "            if not enable_reasoning:\n",
        "                model_lower = (model_name or \"\").lower()\n",
        "                if \"gpt\" in model_lower:\n",
        "                    llm_extra[\"think\"] = \"low\"\n",
        "                else:\n",
        "                    llm_extra[\"think\"] = False\n",
        "        \n",
        "        model_cfg = ModelConfig(\n",
        "            provider=provider,\n",
        "            model_name=model_name,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            timeout=timeout,\n",
        "            base_url=base_url,\n",
        "            extra_params=llm_extra,\n",
        "        )\n",
        "        self.llm_provider = get_provider(model_cfg)\n",
        "\n",
        "        # ---- Initialize embeddings + FAISS ----\n",
        "        self.embeddings, self.embedding_model = initialize_embeddings(\n",
        "            embedding_provider=embedding_provider,\n",
        "            embedding_model=embedding_model,\n",
        "            base_url=base_url,\n",
        "        )\n",
        "\n",
        "        print(f\"Creating vector store with {len(self.knowledge_base.documents)} documents...\")\n",
        "        print(f\"Using {embedding_provider} embeddings ({self.embedding_model})\")\n",
        "        if not self.knowledge_base.documents:\n",
        "            raise ValueError(\"No documents in knowledge base\")\n",
        "\n",
        "        self.vector_store = FAISS.from_documents(self.knowledge_base.documents, self.embeddings)\n",
        "        print(\"✓ Vector store created successfully\")\n",
        "\n",
        "    async def retrieve_and_generate(\n",
        "        self,\n",
        "        question: str,\n",
        "        dataset_type: str,\n",
        "        company: Optional[str] = None,\n",
        "        chains: Optional[List[str]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        try:\n",
        "            print(f\"Retrieving documents for: {question[:50]}...\")\n",
        "            search_terms: List[str] = []\n",
        "            search_term: Optional[str] = None\n",
        "\n",
        "            if dataset_type == \"firm_chains_qa\":\n",
        "                company_match = re.search(r\"公司\\s+([\\w\\*\\-\\+\\(\\)]+)\", question)\n",
        "                search_term = company_match.group(1) if company_match else question\n",
        "                search_terms = [search_term]\n",
        "\n",
        "            elif dataset_type == \"chain_firms_qa\":\n",
        "                chain_match = re.search(r\"產業鏈\\s+([\\w\\*\\-\\+\\(\\)產業鏈]+)\", question)\n",
        "                search_term = chain_match.group(1) if chain_match else question\n",
        "                if isinstance(search_term, str) and search_term.endswith(\"產業鏈產業鏈\"):\n",
        "                    search_term = search_term[:-3]\n",
        "                search_terms = [search_term]\n",
        "\n",
        "            else:  # competitors_qa\n",
        "                if company:\n",
        "                    search_terms.append(company)\n",
        "                if chains:\n",
        "                    search_terms.extend(chains)\n",
        "                if not search_terms:\n",
        "                    search_terms = [question]\n",
        "\n",
        "            scored: List[Tuple[Any, float]] = []\n",
        "            for term in search_terms:\n",
        "                try:\n",
        "                    scored.extend(self.vector_store.similarity_search_with_score(term, k=self.top_k))\n",
        "                except Exception:\n",
        "                    docs = self.vector_store.similarity_search(term, k=self.top_k)\n",
        "                    scored.extend([(d, 0.0) for d in docs])\n",
        "\n",
        "            scored.sort(key=lambda x: x[1])\n",
        "            seen = set()\n",
        "            retrieved_docs = []\n",
        "            for doc, score in scored:\n",
        "                key = (doc.page_content, tuple(sorted(doc.metadata.items())))\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                retrieved_docs.append(doc)\n",
        "                if len(retrieved_docs) >= self.top_k:\n",
        "                    break\n",
        "\n",
        "            print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
        "            if len(retrieved_docs) == 0:\n",
        "                print(f\"No documents retrieved for terms: {search_terms}\")\n",
        "\n",
        "            context = \"\\n\\n\".join([f\"[文件 {i+1}] {doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "            if dataset_type == \"firm_chains_qa\":\n",
        "                system_instruction = \"\"\"你是一位熟悉台灣產業鏈的專家。請根據提供的參考資料回答問題。\n",
        "\n",
        "參考資料：\n",
        "{context}\n",
        "\n",
        "要求：\n",
        "1. 只列出產業鏈的名稱，每個產業鏈一行\n",
        "2. 不要包含編號、項目符號或其他格式\n",
        "3. 產業鏈名稱應該精確，例如「半導體產業鏈」、「電動車產業鏈」等\n",
        "4. 如果參考資料中沒有相關資訊，請回答「根據提供的資料無法確定」\n",
        "5. 不要編造或猜測不在參考資料中的產業鏈\"\"\"\n",
        "\n",
        "            elif dataset_type == \"chain_firms_qa\":\n",
        "                system_instruction = \"\"\"你是一位熟悉台灣產業鏈的專家。請根據提供的參考資料回答問題。\n",
        "\n",
        "參考資料：\n",
        "{context}\n",
        "\n",
        "要求：\n",
        "1. 只列出公司名稱，每個公司一行\n",
        "2. 不要包含編號、項目符號或其他格式\n",
        "3. 公司名稱應該精確，包含台灣本地公司和外國公司\n",
        "4. 如果參考資料中沒有相關資訊，請回答「根據提供的資料無法確定」\n",
        "5. 不要編造或猜測不在參考資料中的公司名稱\"\"\"\n",
        "\n",
        "            else:\n",
        "                system_instruction = \"\"\"你是一位熟悉台灣產業鏈的專家。請根據提供的參考資料回答問題。\n",
        "\n",
        "參考資料：\n",
        "{context}\n",
        "\n",
        "要求：\n",
        "1. 只列出「競爭對手公司名稱」，每個公司一行\n",
        "2. 不要包含編號、項目符號或其他格式\n",
        "3. 公司名稱應該精確，包含台灣本地公司和外國公司\n",
        "4. 若參考資料不足以判斷，請回答「根據提供的資料無法確定」\n",
        "5. 不要編造或猜測不在參考資料中的公司名稱\"\"\"\n",
        "\n",
        "            system_text = system_instruction.format(context=context)\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_text},\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "            ]\n",
        "\n",
        "            print(f\"Sending prompt to {self.provider_name} {self.model_name}...\")\n",
        "            answer = await self.llm_provider.generate(messages)\n",
        "            if answer is None:\n",
        "                raise RuntimeError(\"LLM returned None\")\n",
        "\n",
        "            answer = answer.strip()\n",
        "            print(f\"Received response: {answer[:100]}...\")\n",
        "\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"context\": context,\n",
        "                \"retrieved_docs_count\": len(retrieved_docs),\n",
        "                \"retrieved_docs\": [\n",
        "                    {\n",
        "                        \"content\": (doc.page_content[:200] + \"...\") if len(doc.page_content) > 200 else doc.page_content,\n",
        "                        \"metadata\": doc.metadata,\n",
        "                    }\n",
        "                    for doc in retrieved_docs\n",
        "                ],\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"ERROR in retrieve_and_generate: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": None,\n",
        "                \"error\": str(e),\n",
        "                \"context\": \"\",\n",
        "                \"retrieved_docs_count\": 0,\n",
        "                \"retrieved_docs\": [],\n",
        "            }\n",
        "\n",
        "print(\"✓ RAGModelProvider ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Evaluator (mirrors your script, but uses metrics.py calculate_metrics)\n",
        "\n",
        "class RAGEvaluator:\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.knowledge_base = TaiwanValueChainKnowledgeBase(data_dir=data_dir)\n",
        "        self.dataset_type: Optional[str] = None\n",
        "\n",
        "    def detect_dataset_type(self, sample: Dict[str, Any]) -> str:\n",
        "        if \"company\" in sample and \"chains\" in sample:\n",
        "            return \"competitors_qa\"\n",
        "        if \"company\" in sample:\n",
        "            return \"firm_chains_qa\"\n",
        "        if \"chain\" in sample:\n",
        "            return \"chain_firms_qa\"\n",
        "        raise ValueError(\"Unknown dataset format. Expected 'company' or 'chain' field in data.\")\n",
        "\n",
        "    def parse_response(self, response: Optional[str], dataset_type: str) -> List[str]:\n",
        "        if not response:\n",
        "            return []\n",
        "        uncertainty_patterns = [\n",
        "            r\"不確定\", r\"無法確定\", r\"不知道\", r\"沒有資料\", r\"無法回答\",\n",
        "            r\"資訊不足\", r\"不清楚\", r\"根據提供的資料無法確定\",\n",
        "        ]\n",
        "        for pat in uncertainty_patterns:\n",
        "            if re.search(pat, response, re.IGNORECASE):\n",
        "                return []\n",
        "\n",
        "        lines = response.split(\"\\n\")\n",
        "        items = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            line = re.sub(r\"^[\\d\\.\\-\\*\\•\\→]+\\s*\", \"\", line)\n",
        "            line = re.sub(r\"^\\s*[-·]\\s*\", \"\", line)\n",
        "            line = re.sub(r\"[，。、；：]$\", \"\", line)\n",
        "            if len(line) < 2:\n",
        "                continue\n",
        "\n",
        "            if dataset_type == \"firm_chains_qa\":\n",
        "                if \"產業鏈\" in line:\n",
        "                    items.append(line)\n",
        "            else:\n",
        "                if re.search(r\"[\\u4e00-\\u9fff]\", line) or re.search(r\"[A-Za-z]\", line):\n",
        "                    items.append(line)\n",
        "        return items\n",
        "\n",
        "    async def evaluate_dataset(\n",
        "        self,\n",
        "        dataset_path: str,\n",
        "        rag_provider: RAGModelProvider,\n",
        "        max_samples: Optional[int] = None,\n",
        "        sample_rate: float = 1.0,\n",
        "        save_results: bool = True,\n",
        "        results_dir: Path = Path(\"results\"),\n",
        "    ) -> Dict[str, Any]:\n",
        "\n",
        "        dataset_path = str(dataset_path)\n",
        "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            dataset = [json.loads(line) for line in f]\n",
        "\n",
        "        self.dataset_type = self.detect_dataset_type(dataset[0])\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"Evaluating RAG with {rag_provider.provider_name.upper()} {rag_provider.model_name}\")\n",
        "        print(f\"Dataset Type: {self.dataset_type}\")\n",
        "        print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "        total_samples = len(dataset)\n",
        "        if sample_rate < 1.0:\n",
        "            random.seed(42)\n",
        "            dataset = random.sample(dataset, int(total_samples * sample_rate))\n",
        "        if max_samples:\n",
        "            dataset = dataset[:max_samples]\n",
        "\n",
        "        print(f\"Dataset: {dataset_path}\")\n",
        "        print(f\"Total samples: {total_samples}\")\n",
        "        print(f\"Evaluating: {len(dataset)} samples\")\n",
        "        print(f\"RAG Config: top_k={rag_provider.top_k}, threshold={rag_provider.score_threshold}\")\n",
        "        print(\"\\nStarting RAG evaluation...\\n\")\n",
        "\n",
        "        metrics_list = []\n",
        "        total_exact = 0\n",
        "        error_analysis = {\"api_errors\": 0, \"empty_responses\": 0, \"retrieval_errors\": 0}\n",
        "        detailed_results = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, item in enumerate(dataset, 1):\n",
        "            question = item[\"question\"]\n",
        "            actual_answer = item[\"answer\"]\n",
        "            answer_count = item.get(\"answer_count\", len(actual_answer))\n",
        "\n",
        "            if self.dataset_type == \"firm_chains_qa\":\n",
        "                entity = item[\"company\"]\n",
        "                print(f\"[{idx}/{len(dataset)}] {entity} ({answer_count} chains)...\", end=\" \")\n",
        "            elif self.dataset_type == \"chain_firms_qa\":\n",
        "                entity = item[\"chain\"]\n",
        "                print(f\"[{idx}/{len(dataset)}] {entity} ({answer_count} companies)...\", end=\" \")\n",
        "            else:\n",
        "                entity = item[\"company\"]\n",
        "                chains = item.get(\"chains\", [])\n",
        "                print(f\"[{idx}/{len(dataset)}] {entity} ({answer_count} competitors, {len(chains)} chains)...\", end=\" \")\n",
        "\n",
        "            if self.dataset_type == \"competitors_qa\":\n",
        "                rag_result = await rag_provider.retrieve_and_generate(\n",
        "                    question,\n",
        "                    self.dataset_type,\n",
        "                    company=item.get(\"company\"),\n",
        "                    chains=item.get(\"chains\", []),\n",
        "                )\n",
        "            else:\n",
        "                rag_result = await rag_provider.retrieve_and_generate(question, self.dataset_type)\n",
        "\n",
        "            if rag_result.get(\"error\"):\n",
        "                print(\"❌ RAG Error\")\n",
        "                error_analysis[\"api_errors\"] += 1\n",
        "                response = None\n",
        "                predicted_answer = []\n",
        "            else:\n",
        "                response = rag_result.get(\"answer\")\n",
        "                predicted_answer = self.parse_response(response, self.dataset_type)\n",
        "                if not predicted_answer:\n",
        "                    error_analysis[\"empty_responses\"] += 1\n",
        "                print(f\"✓ ({len(predicted_answer)} predicted, {rag_result.get('retrieved_docs_count', 0)} docs)\")\n",
        "\n",
        "            m = calculate_metrics(predicted_answer, actual_answer)\n",
        "            # mAP/AP (optional) using ordered predicted list\n",
        "            ap = calculate_average_precision(predicted_answer, actual_answer)\n",
        "            try:\n",
        "                m.average_precision = ap\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            metrics_list.append(m)\n",
        "            if m.exact_match == 1.0:\n",
        "                total_exact += 1\n",
        "\n",
        "            result = {\n",
        "                \"index\": idx,\n",
        "                \"entity\": entity,\n",
        "                \"question\": question,\n",
        "                \"actual_answer\": actual_answer,\n",
        "                \"predicted_answer\": predicted_answer,\n",
        "                \"response\": response,\n",
        "                \"rag_context\": rag_result.get(\"context\", \"\"),\n",
        "                \"retrieved_docs_count\": rag_result.get(\"retrieved_docs_count\", 0),\n",
        "                \"retrieved_docs\": rag_result.get(\"retrieved_docs\", []),\n",
        "                \"metrics\": m.to_dict() if hasattr(m, \"to_dict\") else {\n",
        "                    \"recall\": m.recall,\n",
        "                    \"precision\": m.precision,\n",
        "                    \"f1\": m.f1,\n",
        "                    \"exact_match\": m.exact_match,\n",
        "                    \"average_precision\": ap,\n",
        "                },\n",
        "                \"dataset_type\": self.dataset_type,\n",
        "            }\n",
        "\n",
        "            # keep dataset-specific fields\n",
        "            if self.dataset_type == \"firm_chains_qa\":\n",
        "                result[\"company\"] = item.get(\"company\")\n",
        "                result[\"is_foreign\"] = item.get(\"is_foreign\", False)\n",
        "            elif self.dataset_type == \"chain_firms_qa\":\n",
        "                result[\"chain\"] = item.get(\"chain\")\n",
        "                result[\"local_count\"] = item.get(\"local_count\", 0)\n",
        "                result[\"foreign_count\"] = item.get(\"foreign_count\", 0)\n",
        "            else:\n",
        "                result[\"company\"] = item.get(\"company\")\n",
        "                result[\"chains\"] = item.get(\"chains\", [])\n",
        "                result[\"local_count\"] = item.get(\"local_count\", 0)\n",
        "                result[\"foreign_count\"] = item.get(\"foreign_count\", 0)\n",
        "                result[\"is_foreign\"] = item.get(\"is_foreign\", False)\n",
        "\n",
        "            detailed_results.append(result)\n",
        "\n",
        "            if rag_provider.provider_name != \"ollama\":\n",
        "                await asyncio.sleep(0.1)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        n = len(dataset)\n",
        "        mean_recall = sum(m.recall for m in metrics_list) / n if n else 0.0\n",
        "        mean_precision = sum(m.precision for m in metrics_list) / n if n else 0.0\n",
        "        mean_f1 = sum(m.f1 for m in metrics_list) / n if n else 0.0\n",
        "        mean_ap = sum(getattr(m, \"average_precision\", 0.0) for m in metrics_list) / n if n else 0.0\n",
        "\n",
        "        avg_metrics = {\n",
        "            \"recall\": mean_recall,\n",
        "            \"precision\": mean_precision,\n",
        "            \"f1\": mean_f1,\n",
        "            \"map\": mean_ap,\n",
        "            \"exact_match_rate\": (total_exact / n) if n else 0.0,\n",
        "            \"evaluated_samples\": n,\n",
        "            \"total_samples\": total_samples,\n",
        "            \"elapsed_time\": elapsed,\n",
        "            \"avg_time_per_sample\": (elapsed / n) if n else 0.0,\n",
        "        }\n",
        "\n",
        "        full_results = {\n",
        "            \"provider\": rag_provider.provider_name,\n",
        "            \"model\": rag_provider.model_name,\n",
        "            \"embedding_provider\": EMBEDDING_PROVIDER,\n",
        "            \"embedding_model\": rag_provider.embedding_model,\n",
        "            \"rag_config\": {\"top_k\": rag_provider.top_k, \"score_threshold\": rag_provider.score_threshold},\n",
        "            \"dataset\": str(dataset_path),\n",
        "            \"dataset_type\": self.dataset_type,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"average_metrics\": avg_metrics,\n",
        "            \"error_analysis\": error_analysis,\n",
        "            \"detailed_results\": detailed_results,\n",
        "        }\n",
        "\n",
        "        if save_results:\n",
        "            results_dir.mkdir(parents=True, exist_ok=True)\n",
        "            dataset_name = Path(dataset_path).stem\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            model_safe = rag_provider.model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
        "            out = results_dir / f\"rag_evaluation_results_{dataset_name}_{rag_provider.provider_name}_{model_safe}_{timestamp}.json\"\n",
        "            with open(out, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(full_results, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"\\n✓ Detailed results saved to: {out}\")\n",
        "\n",
        "        self.print_summary(full_results)\n",
        "        return full_results\n",
        "\n",
        "    def print_summary(self, results: Dict[str, Any]):\n",
        "        metrics = results[\"average_metrics\"]\n",
        "        errors = results[\"error_analysis\"]\n",
        "        rag_cfg = results[\"rag_config\"]\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"RAG EVALUATION SUMMARY\")\n",
        "        print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "        print(f\"Provider: {results['provider']}\")\n",
        "        print(f\"Model: {results['model']}\")\n",
        "        print(f\"Embedding: {results['embedding_provider']} ({results['embedding_model']})\")\n",
        "        print(f\"RAG Config: top_k={rag_cfg['top_k']}, threshold={rag_cfg['score_threshold']}\")\n",
        "        print(f\"Dataset Type: {results['dataset_type']}\")\n",
        "        print(f\"Evaluated: {metrics['evaluated_samples']} samples\")\n",
        "        print(f\"Time: {metrics['elapsed_time']:.1f}s ({metrics['avg_time_per_sample']:.2f}s per sample)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"RAG PERFORMANCE METRICS\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"  Recall:            {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)\")\n",
        "        print(f\"  Precision:         {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)\")\n",
        "        print(f\"  F1 Score:          {metrics['f1']:.4f} ({metrics['f1']*100:.2f}%)\")\n",
        "        print(f\"  mAP:               {metrics['map']:.4f} ({metrics['map']*100:.2f}%)\")\n",
        "        print(f\"  Exact Match Rate:  {metrics['exact_match_rate']:.4f} ({metrics['exact_match_rate']*100:.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ERROR ANALYSIS\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"  API Errors:        {errors['api_errors']}\")\n",
        "        print(f\"  Empty Responses:   {errors['empty_responses']}\")\n",
        "        print(f\"  Retrieval Errors:  {errors['retrieval_errors']}\")\n",
        "\n",
        "        detailed = results.get(\"detailed_results\", [])\n",
        "        perfect = [r for r in detailed if r.get(\"metrics\", {}).get(\"exact_match\", 0.0) == 1.0]\n",
        "        if perfect:\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(f\"PERFECT PREDICTIONS (showing first 3 of {len(perfect)})\")\n",
        "            print(\"=\" * 70)\n",
        "            for r in perfect[:3]:\n",
        "                entity_name = r.get(\"company\", r.get(\"chain\", r.get(\"entity\")))\n",
        "                print(f\"\\n  Entity: {entity_name}\")\n",
        "                print(f\"  Predicted: {r.get('predicted_answer', [])}\")\n",
        "                print(f\"  Retrieved docs: {r.get('retrieved_docs_count', 0)}\")\n",
        "\n",
        "        partial = [r for r in detailed if 0 < r.get(\"metrics\", {}).get(\"recall\", 0.0) < 1.0]\n",
        "        if partial:\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(f\"PARTIAL MATCHES (showing first 3 of {len(partial)})\")\n",
        "            print(\"=\" * 70)\n",
        "            for r in partial[:3]:\n",
        "                entity_name = r.get(\"company\", r.get(\"chain\", r.get(\"entity\")))\n",
        "                print(f\"\\n  Entity: {entity_name}\")\n",
        "                print(f\"  Actual:    {r.get('actual_answer', [])}\")\n",
        "                print(f\"  Predicted: {r.get('predicted_answer', [])}\")\n",
        "                rec = r.get(\"metrics\", {}).get(\"recall\", 0.0)\n",
        "                prec = r.get(\"metrics\", {}).get(\"precision\", 0.0)\n",
        "                print(f\"  Recall: {rec:.2f}, Precision: {prec:.2f}\")\n",
        "                print(f\"  Retrieved docs: {r.get('retrieved_docs_count', 0)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"✓ RAGEvaluator ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) Run (mirrors __main__ execution)\n",
        "\n",
        "async def run():\n",
        "    # Validate dataset path\n",
        "    if not Path(DATASET_PATH).exists():\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {DATASET_PATH}\")\n",
        "    if not (0.0 < SAMPLE_RATE <= 1.0):\n",
        "        raise ValueError(\"Sample rate must be between 0.0 and 1.0\")\n",
        "\n",
        "    evaluator = RAGEvaluator(data_dir=DATA_DIR)\n",
        "\n",
        "    # Initialize RAG provider\n",
        "    rag_provider = RAGModelProvider(\n",
        "        provider=PROVIDER,\n",
        "        model_name=MODEL_NAME,\n",
        "        temperature=TEMPERATURE,\n",
        "        max_tokens=MAX_TOKENS,\n",
        "        timeout=TIMEOUT,\n",
        "        base_url=os.getenv(\"OLLAMA_BASE_URL\", None),\n",
        "        enable_reasoning=ENABLE_REASONING,\n",
        "        knowledge_base=evaluator.knowledge_base,\n",
        "        embedding_provider=EMBEDDING_PROVIDER,\n",
        "        embedding_model=EMBEDDING_MODEL,\n",
        "        top_k=TOP_K,\n",
        "        score_threshold=SCORE_THRESHOLD,\n",
        "        extra_params={},\n",
        "    )\n",
        "\n",
        "    results = await evaluator.evaluate_dataset(\n",
        "        dataset_path=DATASET_PATH,\n",
        "        rag_provider=rag_provider,\n",
        "        max_samples=MAX_SAMPLES,\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        save_results=SAVE_RESULTS,\n",
        "        results_dir=RESULTS_DIR,\n",
        "    )\n",
        "\n",
        "    print(\"✓ RAG evaluation completed successfully!\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3440399",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jupyter: run with await\n",
        "results = await run()\n",
        "results[\"average_metrics\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "latest_kdd_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
